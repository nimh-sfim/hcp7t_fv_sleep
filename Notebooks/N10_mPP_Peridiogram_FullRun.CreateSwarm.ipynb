{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description - Create SWARM File for Power Spectrum generation for each ROI\n",
    "\n",
    "This notebook will do the following operations:\n",
    "\n",
    "* Compute the spectrograms for each run separately using the Welch method. This part is done in parallel via swarm jobs. The main output for each job is: ```/data/SFIMJGC_HCP7T/HCP7T/${SBJ}/${RUN}/${RUN}.Signal.V4_grp.welch.pkl```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils.variables import Resources_Dir, DATA_DIR\n",
    "from utils.basics import get_available_runs\n",
    "\n",
    "from scipy.stats import kruskal, wilcoxon, ttest_ind\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Name of Runs on each group (Awake, Drowsy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++ INFO: Number of Runs = 404\n"
     ]
    }
   ],
   "source": [
    "Manuscript_Runs = get_available_runs(when='final')\n",
    "print('++ INFO: Number of Runs = %d' % len(Manuscript_Runs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++ INFO: Number of Awake  Runs = 210\n",
      "++ INFO: Number of Drowsy Runs = 194\n"
     ]
    }
   ],
   "source": [
    "Drowsy_Runs = get_available_runs(when='final',type='drowsy')\n",
    "Awake_Runs  = get_available_runs(when='final',type='awake')\n",
    "print('++ INFO: Number of Awake  Runs = %d' % len(Awake_Runs))\n",
    "print('++ INFO: Number of Drowsy Runs = %d' % len(Drowsy_Runs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Swarm Infrastructure\n",
    "\n",
    "These jobs are very fast, less than 5 minutes per run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not osp.exists('./N10_mPP_Peridiogram_FullRun.logs'):\n",
    "    os.mkdir('./N10_mPP_Peridiogram_FullRun.logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Swarm file for extracting representative power\n",
    "# =====================================================\n",
    "os.system('echo \"#swarm -f ./N10_mPP_Peridiogram_FullRun.SWARM.sh -g 16 -t 16 -b 30 --partition quick,norm --time 00:5:00 --logdir ./N10_mPP_Peridiogram_FullRun.logs\" > ./N10_mPP_Peridiogram_FullRun.SWARM.sh')\n",
    "for sbj_run in Manuscript_Runs:\n",
    "    sbj,run  = sbj_run.split('_',1)\n",
    "    out_dir  = osp.join(DATA_DIR,sbj,run)\n",
    "    for region in ['V4_grp','V4lt_grp','V4ut_grp']:\n",
    "        os.system('echo \"export SBJ={sbj} RUN={run} REGION={reg} DATADIR={ddir}; sh ./N10_mPP_Peridiogram_FullRun.sh\" >> ./N10_mPP_Peridiogram_FullRun.SWARM.sh'.format(sbj=sbj, run=run, reg=region, ddir=DATA_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit Jobs to Cluster\n",
    "\n",
    "Login to biowulf, and run the following commands on a terminal:\n",
    "\n",
    "```bash\n",
    "cd /data/SFIMJGC_HCP7T/hcp7t_fv_sleep/Notebooks\n",
    "rm ./N10_mPP_Peridiogram_FullRun.logs/*\n",
    "swarm -f ./N10_mPP_Peridiogram_FullRun.SWARM.sh -g 16 -t 16 --partition quick,norm --time 00:20:00 --logdir ./N10_mPP_Peridiogram_FullRun.logs\n",
    "watch -n 120 squeue -u javiergc\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Test all outputs have been generated by swarm jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++ INFO: Number of available files = 404\n"
     ]
    }
   ],
   "source": [
    "num_files=0\n",
    "suffix='mPP'\n",
    "region='V4ut_grp'\n",
    "for sbj_run in Manuscript_Runs:\n",
    "    sbj,run  = sbj_run.split('_',1)\n",
    "    out_file = osp.join(DATA_DIR,sbj,run,'{run}_{suffix}.Signal.{region}.welch.pkl'.format(run=run, region=region, suffix=suffix))\n",
    "    if not osp.exists(out_file):\n",
    "        print('++ WARNING: File missing [%s]' % out_file)\n",
    "    else:\n",
    "        num_files +=1\n",
    "print('++ INFO: Number of available files = %d' % num_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "hcp7t_fv_sleep_env",
   "language": "python",
   "name": "hcp7t_fv_sleep_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
