{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "This notebook execute all additional pre-processing pipelines that take as input the minimally pre-processed data.\n",
    "\n",
    "This notebook, as previous ones, only sets the swarm file necessary to paralelize this part of the analysis across all runs. The core of the processing code is in ```N06_mPP_postProcessing.sh```. This bash script will perform the following steps for a given run:\n",
    "\n",
    "1. Discard initial 10 seconds from fMRI data\n",
    "\n",
    "2. Spatially smooth the minimally pre-processed data (FWHM = 4)\n",
    "\n",
    "3. Estimate motion DVARS (based on mPP data prior to smoothing)\n",
    "\n",
    "4. Run the Basic Pipeline: blur + bandpass (0.01 - 0.1 Hz) + mot regressors + legendre polynomials.\n",
    "\n",
    "5. Run the Basic Pipeline without the bandpass. This will be the input to the RapidTide software in subsequent steps\n",
    "\n",
    "6. Run the AFNI CompCor Pipeline: blur + bandpass (0.01 - 0.1 Hz) + mot regressors + legendre poly + 3 PCA from lateral ventricles. (Most likely will be discarded in review)\n",
    "\n",
    "7. Run the AFNI CompCor+ Pipeline: blur + bandpass (0.01 - 0.1 Hz) + mot regressors + legendre poly + 3 PCA from lateral ventricles + FV signal. (Most likely will be discarded in review)\n",
    "\n",
    "8. Run the Bezhadi CompCor Pipeline: blur + bandpass (0.01 - 0.1 Hz) + mot regressors + legendre poly + 5 PCA from WM + CSF mask.\n",
    "\n",
    "## Outputs:\n",
    "\n",
    "* ```${DATA_DIR}/${SBJ}/${RUN}/${RUN}_mPP.blur.nii.gz```: Spatially smoothed fMRI data.\n",
    "* ```${DATA_DIR}/${SBJ}/${RUN}/${RUN}_mPP.blur.scale.nii.gz```: Spatially smoothed fMRI data in Signal Percent Change units.\n",
    "* ```${DATA_DIR}/${SBJ}/${RUN}/${RUN}_Movement_SRMS.1D```: Motion in terms of DVARs\n",
    "* ```${DATA_DIR}/${SBJ}/${RUN}/${RUN}_BASIC.nii.gz```: Output of the Basic pre-processing pipeline\n",
    "* ```${DATA_DIR}/${SBJ}/${RUN}/${RUN}_BASICnobpf.nii.gz```: Output of the Basic pre-processing pipeline (no filtering). Only used as input to rapitdite\n",
    "* ```${DATA_DIR}/${SBJ}/${RUN}/${RUN}_AFNI_COMPCOR.nii.gz```: Output from AFNI CompCor pre-processing pipeline\n",
    "* ```${DATA_DIR}/${SBJ}/${RUN}/${RUN}_AFNI_COMPCORp.nii.gz```: Output from AFNI CompCor+ pre-processing pipeline\n",
    "* ```${DATA_DIR}/${SBJ}/${RUN}/${RUN}_Behzadi_COMPCOR.nii.gz```: Output from Behzadi CompCor pre-processing pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "import os\n",
    "from utils.variables import Resources_Dir, DATA_DIR\n",
    "from utils.basics import get_available_runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 1. Load list of runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++ INFO: Number of Runs = 404\n"
     ]
    }
   ],
   "source": [
    "Manuscript_Runs = get_available_runs('final')\n",
    "print('++ INFO: Number of Runs = %d' % len(Manuscript_Runs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# 2. Generate Swarm Infrastructure: log directory, SWARM file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Log Directory for swarm jobs\n",
    "# ===================================\n",
    "if not osp.exists('./N06_mPP_postProcessing.logs'):\n",
    "    print('++ INFO: Creating logging dir')\n",
    "    os.mkdir('./N06_mPP_postProcessing.logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Swarm file for extracting representative power\n",
    "# =====================================================\n",
    "os.system('echo \"#swarm -f ./N06_mPP_postProcessing.SWARM.sh -g 16 -t 16 --partition quick,norm --module afni --logdir ./N06_mPP_postProcessing.logs\" > ./N06_mPP_postProcessing.SWARM.sh')\n",
    "# Add entries regarding periods of eye closure\n",
    "for item in Manuscript_Runs:\n",
    "    sbj,run = item.split('_',1) \n",
    "    os.system('echo \"export SBJ={sbj} RUN={run}; sh ./N06_mPP_postProcessing.sh\" >> ./N06_mPP_postProcessing.SWARM.sh'.format(sbj=sbj, run=run))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# 3. Check all outputs were generated correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.6 ms, sys: 49.3 ms, total: 75 ms\n",
      "Wall time: 577 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for item in Manuscript_Runs:\n",
    "    sbj,run = item.split('_',1)\n",
    "    #for suffix in ['_mPP.blur.nii.gz','_mPP.blur.scale.nii.gz','_Movement_SRMS.1D','_BASIC.nii.gz','_BASICnobpf.nii.gz','_AFNI_COMPCOR.nii.gz','_AFNI_COMPCORp.nii.gz','_Behzadi_COMPCOR.nii.gz']:\n",
    "    for suffix in ['_mPP.blur.nii.gz','_mPP.blur.scale.nii.gz','_Movement_SRMS.1D','_BASIC.nii.gz','_BASICnobpf.nii.gz','_Behzadi_COMPCOR.nii.gz']:\n",
    "        path = osp.join(DATA_DIR,sbj,run,'{run}{suffix}'.format(run=run,suffix=suffix))\n",
    "        if not osp.exists(path):\n",
    "            print('++ WARNING: Missing output [%s]' % path)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "hcp7t_fv_sleep_env",
   "language": "python",
   "name": "hcp7t_fv_sleep_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
